# CLUSTERING

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, fcluster

# Load the feature-engineered dataset
data_df = pd.read_csv('feature_engineered.csv')

# Select the relevant numerical columns for clustering
features = ['Sentiment Score', 'Aggregate Rating', 'Seat Comfort Sentiment Score',
'Food & Beverages Sentiment Score',                                         
'Cabin Staff Service Sentiment Score', 'Food & Beverages',                             
'Seat Comfort',                 
'Cabin Staff Service'                                                               
  ]

# Handle missing values using SimpleImputer
imputer = SimpleImputer(strategy='mean')
features_imputed = imputer.fit_transform(data_df[features])

# Standardize the data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features_imputed)

# Apply PCA for dimensionality reduction (let's reduce to 2 components for visualization)
pca = PCA(n_components=2)
pca_features = pca.fit_transform(scaled_features)

# Add PCA components to the dataframe
data_df['PC1'] = pca_features[:, 0]
data_df['PC2'] = pca_features[:, 1]

# Helper function to visualize clustering results
def visualize_clusters(data, labels, title, centroids=None):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=data['PC1'], y=data['PC2'], hue=labels, palette='viridis', s=100, alpha=0.7, edgecolor='k')
    if centroids is not None:
        plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')
    plt.title(title, fontsize=16)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend(title='Cluster')
    plt.grid(True)
    plt.show()

# Elbow method for optimal k with inertia and silhouette score
inertia = []
silhouette_scores = []
k_range = range(2, 10)  # Start from k=2, as silhouette score is not defined for k=1

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_features)
    
    # Append inertia for elbow method
    inertia.append(kmeans.inertia_)
    
    # Calculate silhouette score and append
    silhouette_avg = silhouette_score(pca_features, kmeans.labels_)
    silhouette_scores.append(silhouette_avg)

optimal_k = 4  # Set based on Elbow Method or Silhouette Score analysis

# Plot Elbow Method results (Inertia)
plt.figure(figsize=(14, 4))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, marker='o', color='b')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k (Inertia)')
plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal k = {optimal_k}')
plt.legend()

# Plot Silhouette Score results
plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, marker='o', color='r')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Optimal k')
plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal k = {optimal_k}')
plt.legend()

plt.tight_layout()
plt.show()

# Perform K-Means Clustering using PCA features
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
data_df['KMeans Cluster'] = kmeans.fit_predict(pca_features)

# K-Means Clustering Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(x=pca_features[:, 0], 
                y=pca_features[:, 1], 
                hue=data_df['KMeans Cluster'].astype(str), 
                palette='deep', 
                s=100, 
                alpha=0.7, 
                edgecolor='w')
plt.title(f'K-Means Clustering (k={optimal_k})', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=14)
plt.ylabel('Principal Component 2', fontsize=14)
plt.legend(title='K-Means Cluster')
plt.grid(True)
plt.show()
# K-Means Evaluation Metrics
print("K-Means Clustering Results:")
print(f"Number of Clusters: {data_df['KMeans Cluster'].nunique()}")
print(f"Inertia: {kmeans.inertia_}")

# Step 1: DBSCAN Clustering using PCA features
dbscan = DBSCAN(eps=0.1, min_samples=2)  # Adjust eps and min_samples as necessary
data_df['DBSCAN Cluster'] = dbscan.fit_predict(pca_features)

# DBSCAN Clustering Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(x=pca_features[:, 0], 
                y=pca_features[:, 1], 
                hue=data_df['DBSCAN Cluster'].astype(str), 
                palette='deep', 
                s=100, 
                alpha=0.7, 
                edgecolor='w',
                legend=False)  # Disable the legend

plt.title('DBSCAN Clustering of Airline Reviews (PCA)', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=14)
plt.ylabel('Principal Component 2', fontsize=14)
plt.grid(True)
plt.show()

# Print DBSCAN Summary
print("\nDBSCAN Clustering Results:")
print("Number of Clusters:", data_df['DBSCAN Cluster'].nunique() - 1)  # Subtract 1 for noise points (-1)

# Step 2: Agglomerative Clustering (Hierarchical) using PCA features
linkage_matrix = linkage(pca_features, method='ward')

# Cut the dendrogram to create clusters
threshold = 15  # Adjust this threshold based on your dendrogram if needed
data_df['Agglomerative Cluster'] = fcluster(linkage_matrix, threshold, criterion='distance')

# Agglomerative Clustering Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(x=pca_features[:, 0], 
                y=pca_features[:, 1], 
                hue=data_df['Agglomerative Cluster'], 
                palette='deep', 
                s=100, 
                alpha=0.7, 
                edgecolor='w')

plt.title('Agglomerative Clustering of Airline Reviews (PCA)', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=14)
plt.ylabel('Principal Component 2', fontsize=14)
plt.legend(title='Agglomerative Cluster')
plt.grid(True)
plt.show()

# Print Agglomerative Summary
print("\nAgglomerative Clustering Results:")
print("Number of Clusters:", data_df['Agglomerative Cluster'].nunique())

# Divisive Hierarchical Clustering (AgglomerativeClustering) using PCA features
agglom = AgglomerativeClustering(n_clusters=2, linkage='ward')

# Assign divisive cluster labels using .loc to avoid SettingWithCopyWarning
data_df.loc[:, 'Divisive Cluster'] = agglom.fit_predict(pca_features)

# Divisive Clustering Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(x=pca_features[:, 0], 
                y=pca_features[:, 1], 
                hue=data_df['Divisive Cluster'], 
                palette='deep', 
                s=100, 
                alpha=0.7, 
                edgecolor='w')

plt.title('Divisive Clustering of Airline Reviews (PCA)', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=14)
plt.ylabel('Principal Component 2', fontsize=14)
plt.legend(title='Divisive Cluster')
plt.grid(True)
plt.show()

# Print Divisive Summary
print("\nDivisive Clustering Results:")
print("Number of Clusters:", data_df['Divisive Cluster'].nunique())

print("\nSILHOUTTE SCORES:")

# K-Means Silhouette Score
kmeans_silhouette = silhouette_score(pca_features, data_df['KMeans Cluster'])
print(f"\nK-Means Silhouette Score: {round(kmeans_silhouette, 3)}")

# DBSCAN Silhouette Score (exclude noise points (-1) from both pca_features and labels)
dbscan_labels = data_df['DBSCAN Cluster']
dbscan_mask = dbscan_labels != -1  # Mask to exclude noise points
dbscan_silhouette = silhouette_score(pca_features[dbscan_mask], dbscan_labels[dbscan_mask])
print(f"\nDBSCAN Silhouette Score: {round(dbscan_silhouette, 3)}")

# Agglomerative Clustering Silhouette Score
agglo_silhouette = silhouette_score(pca_features, data_df['Agglomerative Cluster'])
print(f"\nAgglomerative Clustering Silhouette Score: {round(agglo_silhouette, 3)}")

# Divisive Clustering Silhouette Score
divisive_silhouette = silhouette_score(pca_features, data_df['Divisive Cluster'])
print(f"\nDivisive Clustering Silhouette Score: {round(divisive_silhouette, 3)}")

# Compare and print the best clustering algorithm
silhouette_scores = {
    'K-Means': kmeans_silhouette,
    'DBSCAN': dbscan_silhouette,
    'Agglomerative': agglo_silhouette,
    'Divisive': divisive_silhouette
}

# Find the best algorithm with the highest silhouette score
best_algorithm = max(silhouette_scores, key=silhouette_scores.get)
print(f"\nBest Algorithm based on Silhouette Score: {best_algorithm}")
